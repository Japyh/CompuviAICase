{
  "best_metric": 0.46290302153434176,
  "best_model_checkpoint": "models\\checkpoint-1038",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1038,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.019267822736030827,
      "grad_norm": 3.341149091720581,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 2.0895,
      "step": 10
    },
    {
      "epoch": 0.038535645472061654,
      "grad_norm": 2.9493298530578613,
      "learning_rate": 8.000000000000001e-07,
      "loss": 2.0808,
      "step": 20
    },
    {
      "epoch": 0.057803468208092484,
      "grad_norm": 2.2529797554016113,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 2.0582,
      "step": 30
    },
    {
      "epoch": 0.07707129094412331,
      "grad_norm": 3.15909743309021,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 2.0553,
      "step": 40
    },
    {
      "epoch": 0.09633911368015415,
      "grad_norm": 2.4694955348968506,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 2.0266,
      "step": 50
    },
    {
      "epoch": 0.11560693641618497,
      "grad_norm": 2.9034435749053955,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 2.002,
      "step": 60
    },
    {
      "epoch": 0.1348747591522158,
      "grad_norm": 2.1788368225097656,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 1.9612,
      "step": 70
    },
    {
      "epoch": 0.15414258188824662,
      "grad_norm": 2.221971035003662,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 1.9229,
      "step": 80
    },
    {
      "epoch": 0.17341040462427745,
      "grad_norm": 3.3078949451446533,
      "learning_rate": 3.6000000000000003e-06,
      "loss": 1.8727,
      "step": 90
    },
    {
      "epoch": 0.1926782273603083,
      "grad_norm": 3.15514874458313,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.9056,
      "step": 100
    },
    {
      "epoch": 0.2119460500963391,
      "grad_norm": 3.879648447036743,
      "learning_rate": 4.4e-06,
      "loss": 1.9075,
      "step": 110
    },
    {
      "epoch": 0.23121387283236994,
      "grad_norm": 2.7769381999969482,
      "learning_rate": 4.800000000000001e-06,
      "loss": 1.8415,
      "step": 120
    },
    {
      "epoch": 0.2504816955684008,
      "grad_norm": 2.5186734199523926,
      "learning_rate": 5.2e-06,
      "loss": 1.8847,
      "step": 130
    },
    {
      "epoch": 0.2697495183044316,
      "grad_norm": 2.4649202823638916,
      "learning_rate": 5.600000000000001e-06,
      "loss": 1.8035,
      "step": 140
    },
    {
      "epoch": 0.28901734104046245,
      "grad_norm": 2.282503604888916,
      "learning_rate": 6e-06,
      "loss": 1.7992,
      "step": 150
    },
    {
      "epoch": 0.30828516377649323,
      "grad_norm": 2.8002796173095703,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 1.8547,
      "step": 160
    },
    {
      "epoch": 0.32755298651252407,
      "grad_norm": 2.670259475708008,
      "learning_rate": 6.800000000000001e-06,
      "loss": 1.9341,
      "step": 170
    },
    {
      "epoch": 0.3468208092485549,
      "grad_norm": 2.4652957916259766,
      "learning_rate": 7.2000000000000005e-06,
      "loss": 1.7353,
      "step": 180
    },
    {
      "epoch": 0.36608863198458574,
      "grad_norm": 2.303405523300171,
      "learning_rate": 7.600000000000001e-06,
      "loss": 1.7876,
      "step": 190
    },
    {
      "epoch": 0.3853564547206166,
      "grad_norm": 4.588386058807373,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.7434,
      "step": 200
    },
    {
      "epoch": 0.4046242774566474,
      "grad_norm": 3.1738600730895996,
      "learning_rate": 8.400000000000001e-06,
      "loss": 1.7987,
      "step": 210
    },
    {
      "epoch": 0.4238921001926782,
      "grad_norm": 2.9630627632141113,
      "learning_rate": 8.8e-06,
      "loss": 1.8568,
      "step": 220
    },
    {
      "epoch": 0.44315992292870904,
      "grad_norm": 8.060901641845703,
      "learning_rate": 9.200000000000002e-06,
      "loss": 1.7571,
      "step": 230
    },
    {
      "epoch": 0.4624277456647399,
      "grad_norm": 3.8732943534851074,
      "learning_rate": 9.600000000000001e-06,
      "loss": 1.6441,
      "step": 240
    },
    {
      "epoch": 0.4816955684007707,
      "grad_norm": 4.9471330642700195,
      "learning_rate": 1e-05,
      "loss": 1.7086,
      "step": 250
    },
    {
      "epoch": 0.5009633911368016,
      "grad_norm": 28.627517700195312,
      "learning_rate": 1.04e-05,
      "loss": 1.6457,
      "step": 260
    },
    {
      "epoch": 0.5202312138728323,
      "grad_norm": 6.524630546569824,
      "learning_rate": 1.0800000000000002e-05,
      "loss": 1.4851,
      "step": 270
    },
    {
      "epoch": 0.5394990366088632,
      "grad_norm": 5.8667521476745605,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 1.6042,
      "step": 280
    },
    {
      "epoch": 0.558766859344894,
      "grad_norm": 5.464172840118408,
      "learning_rate": 1.16e-05,
      "loss": 1.5096,
      "step": 290
    },
    {
      "epoch": 0.5780346820809249,
      "grad_norm": 7.521273136138916,
      "learning_rate": 1.2e-05,
      "loss": 1.5598,
      "step": 300
    },
    {
      "epoch": 0.5973025048169557,
      "grad_norm": 5.51540470123291,
      "learning_rate": 1.2400000000000002e-05,
      "loss": 1.5313,
      "step": 310
    },
    {
      "epoch": 0.6165703275529865,
      "grad_norm": 40.15625762939453,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 1.6156,
      "step": 320
    },
    {
      "epoch": 0.6358381502890174,
      "grad_norm": 21.683130264282227,
      "learning_rate": 1.3200000000000002e-05,
      "loss": 1.6475,
      "step": 330
    },
    {
      "epoch": 0.6551059730250481,
      "grad_norm": 11.67081069946289,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 1.3857,
      "step": 340
    },
    {
      "epoch": 0.674373795761079,
      "grad_norm": 12.573092460632324,
      "learning_rate": 1.4e-05,
      "loss": 1.3822,
      "step": 350
    },
    {
      "epoch": 0.6936416184971098,
      "grad_norm": 2.510951042175293,
      "learning_rate": 1.4400000000000001e-05,
      "loss": 1.409,
      "step": 360
    },
    {
      "epoch": 0.7129094412331407,
      "grad_norm": 14.122943878173828,
      "learning_rate": 1.48e-05,
      "loss": 1.2898,
      "step": 370
    },
    {
      "epoch": 0.7321772639691715,
      "grad_norm": 5.102781295776367,
      "learning_rate": 1.5200000000000002e-05,
      "loss": 1.5348,
      "step": 380
    },
    {
      "epoch": 0.7514450867052023,
      "grad_norm": 11.761713981628418,
      "learning_rate": 1.5600000000000003e-05,
      "loss": 1.5302,
      "step": 390
    },
    {
      "epoch": 0.7707129094412332,
      "grad_norm": 13.01358699798584,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.3199,
      "step": 400
    },
    {
      "epoch": 0.789980732177264,
      "grad_norm": 2.8890457153320312,
      "learning_rate": 1.64e-05,
      "loss": 1.3601,
      "step": 410
    },
    {
      "epoch": 0.8092485549132948,
      "grad_norm": 4.175000190734863,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 1.3814,
      "step": 420
    },
    {
      "epoch": 0.8285163776493256,
      "grad_norm": 8.68525218963623,
      "learning_rate": 1.72e-05,
      "loss": 1.4598,
      "step": 430
    },
    {
      "epoch": 0.8477842003853564,
      "grad_norm": 8.007611274719238,
      "learning_rate": 1.76e-05,
      "loss": 1.3324,
      "step": 440
    },
    {
      "epoch": 0.8670520231213873,
      "grad_norm": 23.889366149902344,
      "learning_rate": 1.8e-05,
      "loss": 1.4375,
      "step": 450
    },
    {
      "epoch": 0.8863198458574181,
      "grad_norm": 8.706695556640625,
      "learning_rate": 1.8400000000000003e-05,
      "loss": 1.3413,
      "step": 460
    },
    {
      "epoch": 0.905587668593449,
      "grad_norm": 6.013406753540039,
      "learning_rate": 1.88e-05,
      "loss": 1.3924,
      "step": 470
    },
    {
      "epoch": 0.9248554913294798,
      "grad_norm": 12.740026473999023,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 1.2405,
      "step": 480
    },
    {
      "epoch": 0.9441233140655106,
      "grad_norm": 9.144970893859863,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 1.3687,
      "step": 490
    },
    {
      "epoch": 0.9633911368015414,
      "grad_norm": 7.640869140625,
      "learning_rate": 2e-05,
      "loss": 1.2847,
      "step": 500
    },
    {
      "epoch": 0.9826589595375722,
      "grad_norm": 27.0850772857666,
      "learning_rate": 1.981078524124882e-05,
      "loss": 1.1902,
      "step": 510
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.5511811023622047,
      "eval_f1": 0.22629709837942757,
      "eval_loss": 1.4209948778152466,
      "eval_precision": 0.18974597991259445,
      "eval_recall": 0.2818523997370151,
      "eval_runtime": 525.2431,
      "eval_samples_per_second": 3.385,
      "eval_steps_per_second": 0.213,
      "step": 519
    },
    {
      "epoch": 1.001926782273603,
      "grad_norm": 29.822004318237305,
      "learning_rate": 1.9621570482497638e-05,
      "loss": 1.1106,
      "step": 520
    },
    {
      "epoch": 1.0211946050096339,
      "grad_norm": 16.7353458404541,
      "learning_rate": 1.9432355723746455e-05,
      "loss": 1.2686,
      "step": 530
    },
    {
      "epoch": 1.0404624277456647,
      "grad_norm": 21.122045516967773,
      "learning_rate": 1.924314096499527e-05,
      "loss": 1.2182,
      "step": 540
    },
    {
      "epoch": 1.0597302504816957,
      "grad_norm": 21.372201919555664,
      "learning_rate": 1.9053926206244088e-05,
      "loss": 1.3181,
      "step": 550
    },
    {
      "epoch": 1.0789980732177264,
      "grad_norm": 32.59337615966797,
      "learning_rate": 1.8864711447492906e-05,
      "loss": 1.1758,
      "step": 560
    },
    {
      "epoch": 1.0982658959537572,
      "grad_norm": 12.95024585723877,
      "learning_rate": 1.8675496688741724e-05,
      "loss": 1.1647,
      "step": 570
    },
    {
      "epoch": 1.117533718689788,
      "grad_norm": 17.402019500732422,
      "learning_rate": 1.8486281929990542e-05,
      "loss": 1.2214,
      "step": 580
    },
    {
      "epoch": 1.1368015414258188,
      "grad_norm": 21.05204200744629,
      "learning_rate": 1.8297067171239356e-05,
      "loss": 1.2264,
      "step": 590
    },
    {
      "epoch": 1.1560693641618498,
      "grad_norm": 15.518013954162598,
      "learning_rate": 1.8107852412488174e-05,
      "loss": 1.1662,
      "step": 600
    },
    {
      "epoch": 1.1753371868978806,
      "grad_norm": 8.771515846252441,
      "learning_rate": 1.7918637653736992e-05,
      "loss": 1.2937,
      "step": 610
    },
    {
      "epoch": 1.1946050096339114,
      "grad_norm": 10.275175094604492,
      "learning_rate": 1.772942289498581e-05,
      "loss": 1.2218,
      "step": 620
    },
    {
      "epoch": 1.2138728323699421,
      "grad_norm": 11.251815795898438,
      "learning_rate": 1.7540208136234628e-05,
      "loss": 1.1777,
      "step": 630
    },
    {
      "epoch": 1.2331406551059731,
      "grad_norm": 6.669099807739258,
      "learning_rate": 1.7350993377483446e-05,
      "loss": 1.165,
      "step": 640
    },
    {
      "epoch": 1.252408477842004,
      "grad_norm": 3.119745969772339,
      "learning_rate": 1.7161778618732264e-05,
      "loss": 1.2575,
      "step": 650
    },
    {
      "epoch": 1.2716763005780347,
      "grad_norm": 2.6760144233703613,
      "learning_rate": 1.697256385998108e-05,
      "loss": 1.0148,
      "step": 660
    },
    {
      "epoch": 1.2909441233140655,
      "grad_norm": 3.274334192276001,
      "learning_rate": 1.6783349101229897e-05,
      "loss": 1.2336,
      "step": 670
    },
    {
      "epoch": 1.3102119460500963,
      "grad_norm": 5.98309850692749,
      "learning_rate": 1.6594134342478715e-05,
      "loss": 1.2053,
      "step": 680
    },
    {
      "epoch": 1.3294797687861273,
      "grad_norm": 6.447481155395508,
      "learning_rate": 1.6404919583727533e-05,
      "loss": 1.3016,
      "step": 690
    },
    {
      "epoch": 1.348747591522158,
      "grad_norm": 14.914405822753906,
      "learning_rate": 1.621570482497635e-05,
      "loss": 1.3413,
      "step": 700
    },
    {
      "epoch": 1.3680154142581888,
      "grad_norm": 5.219298839569092,
      "learning_rate": 1.6026490066225165e-05,
      "loss": 1.251,
      "step": 710
    },
    {
      "epoch": 1.3872832369942196,
      "grad_norm": 9.948593139648438,
      "learning_rate": 1.5837275307473986e-05,
      "loss": 0.8498,
      "step": 720
    },
    {
      "epoch": 1.4065510597302504,
      "grad_norm": 10.137024879455566,
      "learning_rate": 1.56480605487228e-05,
      "loss": 1.2503,
      "step": 730
    },
    {
      "epoch": 1.4258188824662814,
      "grad_norm": 23.266035079956055,
      "learning_rate": 1.545884578997162e-05,
      "loss": 1.0032,
      "step": 740
    },
    {
      "epoch": 1.4450867052023122,
      "grad_norm": 10.30461311340332,
      "learning_rate": 1.5269631031220437e-05,
      "loss": 1.0758,
      "step": 750
    },
    {
      "epoch": 1.464354527938343,
      "grad_norm": 13.387009620666504,
      "learning_rate": 1.5080416272469253e-05,
      "loss": 1.1639,
      "step": 760
    },
    {
      "epoch": 1.4836223506743738,
      "grad_norm": 10.739779472351074,
      "learning_rate": 1.4891201513718073e-05,
      "loss": 0.9573,
      "step": 770
    },
    {
      "epoch": 1.5028901734104045,
      "grad_norm": 15.623394012451172,
      "learning_rate": 1.4701986754966889e-05,
      "loss": 0.9984,
      "step": 780
    },
    {
      "epoch": 1.5221579961464355,
      "grad_norm": 6.868142604827881,
      "learning_rate": 1.4512771996215707e-05,
      "loss": 0.986,
      "step": 790
    },
    {
      "epoch": 1.5414258188824663,
      "grad_norm": 4.096749782562256,
      "learning_rate": 1.4323557237464523e-05,
      "loss": 1.1721,
      "step": 800
    },
    {
      "epoch": 1.560693641618497,
      "grad_norm": 20.28993797302246,
      "learning_rate": 1.4134342478713341e-05,
      "loss": 1.0794,
      "step": 810
    },
    {
      "epoch": 1.579961464354528,
      "grad_norm": 4.649950981140137,
      "learning_rate": 1.3945127719962157e-05,
      "loss": 1.1198,
      "step": 820
    },
    {
      "epoch": 1.5992292870905587,
      "grad_norm": 10.071159362792969,
      "learning_rate": 1.3755912961210975e-05,
      "loss": 1.0985,
      "step": 830
    },
    {
      "epoch": 1.6184971098265897,
      "grad_norm": 4.04088020324707,
      "learning_rate": 1.3566698202459793e-05,
      "loss": 1.2192,
      "step": 840
    },
    {
      "epoch": 1.6377649325626205,
      "grad_norm": 11.342137336730957,
      "learning_rate": 1.337748344370861e-05,
      "loss": 1.1253,
      "step": 850
    },
    {
      "epoch": 1.6570327552986512,
      "grad_norm": 11.1618013381958,
      "learning_rate": 1.3188268684957428e-05,
      "loss": 1.1722,
      "step": 860
    },
    {
      "epoch": 1.6763005780346822,
      "grad_norm": 6.562044143676758,
      "learning_rate": 1.2999053926206244e-05,
      "loss": 1.0318,
      "step": 870
    },
    {
      "epoch": 1.6955684007707128,
      "grad_norm": 14.16800594329834,
      "learning_rate": 1.2809839167455063e-05,
      "loss": 1.0963,
      "step": 880
    },
    {
      "epoch": 1.7148362235067438,
      "grad_norm": 4.144528865814209,
      "learning_rate": 1.2620624408703881e-05,
      "loss": 1.0971,
      "step": 890
    },
    {
      "epoch": 1.7341040462427746,
      "grad_norm": 13.979485511779785,
      "learning_rate": 1.2431409649952698e-05,
      "loss": 1.1821,
      "step": 900
    },
    {
      "epoch": 1.7533718689788054,
      "grad_norm": 7.599876880645752,
      "learning_rate": 1.2242194891201516e-05,
      "loss": 1.2304,
      "step": 910
    },
    {
      "epoch": 1.7726396917148364,
      "grad_norm": 14.6217679977417,
      "learning_rate": 1.2052980132450332e-05,
      "loss": 0.9031,
      "step": 920
    },
    {
      "epoch": 1.791907514450867,
      "grad_norm": 5.852708339691162,
      "learning_rate": 1.186376537369915e-05,
      "loss": 1.0619,
      "step": 930
    },
    {
      "epoch": 1.811175337186898,
      "grad_norm": 8.71501350402832,
      "learning_rate": 1.1674550614947966e-05,
      "loss": 1.0173,
      "step": 940
    },
    {
      "epoch": 1.8304431599229287,
      "grad_norm": 3.5620572566986084,
      "learning_rate": 1.1485335856196784e-05,
      "loss": 1.1032,
      "step": 950
    },
    {
      "epoch": 1.8497109826589595,
      "grad_norm": 10.31287670135498,
      "learning_rate": 1.1296121097445602e-05,
      "loss": 1.0799,
      "step": 960
    },
    {
      "epoch": 1.8689788053949905,
      "grad_norm": 3.304008722305298,
      "learning_rate": 1.1106906338694418e-05,
      "loss": 0.9621,
      "step": 970
    },
    {
      "epoch": 1.888246628131021,
      "grad_norm": 6.720941066741943,
      "learning_rate": 1.0917691579943238e-05,
      "loss": 1.0991,
      "step": 980
    },
    {
      "epoch": 1.907514450867052,
      "grad_norm": 12.91418743133545,
      "learning_rate": 1.0728476821192052e-05,
      "loss": 0.8759,
      "step": 990
    },
    {
      "epoch": 1.9267822736030829,
      "grad_norm": 6.608465194702148,
      "learning_rate": 1.0539262062440872e-05,
      "loss": 0.8558,
      "step": 1000
    },
    {
      "epoch": 1.9460500963391136,
      "grad_norm": 9.919888496398926,
      "learning_rate": 1.0350047303689688e-05,
      "loss": 1.0525,
      "step": 1010
    },
    {
      "epoch": 1.9653179190751446,
      "grad_norm": 8.244749069213867,
      "learning_rate": 1.0160832544938506e-05,
      "loss": 0.9894,
      "step": 1020
    },
    {
      "epoch": 1.9845857418111752,
      "grad_norm": 8.927614212036133,
      "learning_rate": 9.971617786187322e-06,
      "loss": 1.0052,
      "step": 1030
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.6569178852643419,
      "eval_f1": 0.46290302153434176,
      "eval_loss": 0.9644736647605896,
      "eval_precision": 0.5849995805518844,
      "eval_recall": 0.4925900563079422,
      "eval_runtime": 520.0278,
      "eval_samples_per_second": 3.419,
      "eval_steps_per_second": 0.215,
      "step": 1038
    }
  ],
  "logging_steps": 10,
  "max_steps": 1557,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2197339367522304.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
