{
  "best_metric": 0.6395258057827344,
  "best_model_checkpoint": "models\\checkpoint-1557",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1557,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.019267822736030827,
      "grad_norm": 3.341149091720581,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 2.0895,
      "step": 10
    },
    {
      "epoch": 0.038535645472061654,
      "grad_norm": 2.9493298530578613,
      "learning_rate": 8.000000000000001e-07,
      "loss": 2.0808,
      "step": 20
    },
    {
      "epoch": 0.057803468208092484,
      "grad_norm": 2.2529797554016113,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 2.0582,
      "step": 30
    },
    {
      "epoch": 0.07707129094412331,
      "grad_norm": 3.15909743309021,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 2.0553,
      "step": 40
    },
    {
      "epoch": 0.09633911368015415,
      "grad_norm": 2.4694955348968506,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 2.0266,
      "step": 50
    },
    {
      "epoch": 0.11560693641618497,
      "grad_norm": 2.9034435749053955,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 2.002,
      "step": 60
    },
    {
      "epoch": 0.1348747591522158,
      "grad_norm": 2.1788368225097656,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 1.9612,
      "step": 70
    },
    {
      "epoch": 0.15414258188824662,
      "grad_norm": 2.221971035003662,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 1.9229,
      "step": 80
    },
    {
      "epoch": 0.17341040462427745,
      "grad_norm": 3.3078949451446533,
      "learning_rate": 3.6000000000000003e-06,
      "loss": 1.8727,
      "step": 90
    },
    {
      "epoch": 0.1926782273603083,
      "grad_norm": 3.15514874458313,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.9056,
      "step": 100
    },
    {
      "epoch": 0.2119460500963391,
      "grad_norm": 3.879648447036743,
      "learning_rate": 4.4e-06,
      "loss": 1.9075,
      "step": 110
    },
    {
      "epoch": 0.23121387283236994,
      "grad_norm": 2.7769381999969482,
      "learning_rate": 4.800000000000001e-06,
      "loss": 1.8415,
      "step": 120
    },
    {
      "epoch": 0.2504816955684008,
      "grad_norm": 2.5186734199523926,
      "learning_rate": 5.2e-06,
      "loss": 1.8847,
      "step": 130
    },
    {
      "epoch": 0.2697495183044316,
      "grad_norm": 2.4649202823638916,
      "learning_rate": 5.600000000000001e-06,
      "loss": 1.8035,
      "step": 140
    },
    {
      "epoch": 0.28901734104046245,
      "grad_norm": 2.282503604888916,
      "learning_rate": 6e-06,
      "loss": 1.7992,
      "step": 150
    },
    {
      "epoch": 0.30828516377649323,
      "grad_norm": 2.8002796173095703,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 1.8547,
      "step": 160
    },
    {
      "epoch": 0.32755298651252407,
      "grad_norm": 2.670259475708008,
      "learning_rate": 6.800000000000001e-06,
      "loss": 1.9341,
      "step": 170
    },
    {
      "epoch": 0.3468208092485549,
      "grad_norm": 2.4652957916259766,
      "learning_rate": 7.2000000000000005e-06,
      "loss": 1.7353,
      "step": 180
    },
    {
      "epoch": 0.36608863198458574,
      "grad_norm": 2.303405523300171,
      "learning_rate": 7.600000000000001e-06,
      "loss": 1.7876,
      "step": 190
    },
    {
      "epoch": 0.3853564547206166,
      "grad_norm": 4.588386058807373,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.7434,
      "step": 200
    },
    {
      "epoch": 0.4046242774566474,
      "grad_norm": 3.1738600730895996,
      "learning_rate": 8.400000000000001e-06,
      "loss": 1.7987,
      "step": 210
    },
    {
      "epoch": 0.4238921001926782,
      "grad_norm": 2.9630627632141113,
      "learning_rate": 8.8e-06,
      "loss": 1.8568,
      "step": 220
    },
    {
      "epoch": 0.44315992292870904,
      "grad_norm": 8.060901641845703,
      "learning_rate": 9.200000000000002e-06,
      "loss": 1.7571,
      "step": 230
    },
    {
      "epoch": 0.4624277456647399,
      "grad_norm": 3.8732943534851074,
      "learning_rate": 9.600000000000001e-06,
      "loss": 1.6441,
      "step": 240
    },
    {
      "epoch": 0.4816955684007707,
      "grad_norm": 4.9471330642700195,
      "learning_rate": 1e-05,
      "loss": 1.7086,
      "step": 250
    },
    {
      "epoch": 0.5009633911368016,
      "grad_norm": 28.627517700195312,
      "learning_rate": 1.04e-05,
      "loss": 1.6457,
      "step": 260
    },
    {
      "epoch": 0.5202312138728323,
      "grad_norm": 6.524630546569824,
      "learning_rate": 1.0800000000000002e-05,
      "loss": 1.4851,
      "step": 270
    },
    {
      "epoch": 0.5394990366088632,
      "grad_norm": 5.8667521476745605,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 1.6042,
      "step": 280
    },
    {
      "epoch": 0.558766859344894,
      "grad_norm": 5.464172840118408,
      "learning_rate": 1.16e-05,
      "loss": 1.5096,
      "step": 290
    },
    {
      "epoch": 0.5780346820809249,
      "grad_norm": 7.521273136138916,
      "learning_rate": 1.2e-05,
      "loss": 1.5598,
      "step": 300
    },
    {
      "epoch": 0.5973025048169557,
      "grad_norm": 5.51540470123291,
      "learning_rate": 1.2400000000000002e-05,
      "loss": 1.5313,
      "step": 310
    },
    {
      "epoch": 0.6165703275529865,
      "grad_norm": 40.15625762939453,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 1.6156,
      "step": 320
    },
    {
      "epoch": 0.6358381502890174,
      "grad_norm": 21.683130264282227,
      "learning_rate": 1.3200000000000002e-05,
      "loss": 1.6475,
      "step": 330
    },
    {
      "epoch": 0.6551059730250481,
      "grad_norm": 11.67081069946289,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 1.3857,
      "step": 340
    },
    {
      "epoch": 0.674373795761079,
      "grad_norm": 12.573092460632324,
      "learning_rate": 1.4e-05,
      "loss": 1.3822,
      "step": 350
    },
    {
      "epoch": 0.6936416184971098,
      "grad_norm": 2.510951042175293,
      "learning_rate": 1.4400000000000001e-05,
      "loss": 1.409,
      "step": 360
    },
    {
      "epoch": 0.7129094412331407,
      "grad_norm": 14.122943878173828,
      "learning_rate": 1.48e-05,
      "loss": 1.2898,
      "step": 370
    },
    {
      "epoch": 0.7321772639691715,
      "grad_norm": 5.102781295776367,
      "learning_rate": 1.5200000000000002e-05,
      "loss": 1.5348,
      "step": 380
    },
    {
      "epoch": 0.7514450867052023,
      "grad_norm": 11.761713981628418,
      "learning_rate": 1.5600000000000003e-05,
      "loss": 1.5302,
      "step": 390
    },
    {
      "epoch": 0.7707129094412332,
      "grad_norm": 13.01358699798584,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.3199,
      "step": 400
    },
    {
      "epoch": 0.789980732177264,
      "grad_norm": 2.8890457153320312,
      "learning_rate": 1.64e-05,
      "loss": 1.3601,
      "step": 410
    },
    {
      "epoch": 0.8092485549132948,
      "grad_norm": 4.175000190734863,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 1.3814,
      "step": 420
    },
    {
      "epoch": 0.8285163776493256,
      "grad_norm": 8.68525218963623,
      "learning_rate": 1.72e-05,
      "loss": 1.4598,
      "step": 430
    },
    {
      "epoch": 0.8477842003853564,
      "grad_norm": 8.007611274719238,
      "learning_rate": 1.76e-05,
      "loss": 1.3324,
      "step": 440
    },
    {
      "epoch": 0.8670520231213873,
      "grad_norm": 23.889366149902344,
      "learning_rate": 1.8e-05,
      "loss": 1.4375,
      "step": 450
    },
    {
      "epoch": 0.8863198458574181,
      "grad_norm": 8.706695556640625,
      "learning_rate": 1.8400000000000003e-05,
      "loss": 1.3413,
      "step": 460
    },
    {
      "epoch": 0.905587668593449,
      "grad_norm": 6.013406753540039,
      "learning_rate": 1.88e-05,
      "loss": 1.3924,
      "step": 470
    },
    {
      "epoch": 0.9248554913294798,
      "grad_norm": 12.740026473999023,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 1.2405,
      "step": 480
    },
    {
      "epoch": 0.9441233140655106,
      "grad_norm": 9.144970893859863,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 1.3687,
      "step": 490
    },
    {
      "epoch": 0.9633911368015414,
      "grad_norm": 7.640869140625,
      "learning_rate": 2e-05,
      "loss": 1.2847,
      "step": 500
    },
    {
      "epoch": 0.9826589595375722,
      "grad_norm": 27.0850772857666,
      "learning_rate": 1.981078524124882e-05,
      "loss": 1.1902,
      "step": 510
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.5511811023622047,
      "eval_f1": 0.22629709837942757,
      "eval_loss": 1.4209948778152466,
      "eval_precision": 0.18974597991259445,
      "eval_recall": 0.2818523997370151,
      "eval_runtime": 525.2431,
      "eval_samples_per_second": 3.385,
      "eval_steps_per_second": 0.213,
      "step": 519
    },
    {
      "epoch": 1.001926782273603,
      "grad_norm": 29.822004318237305,
      "learning_rate": 1.9621570482497638e-05,
      "loss": 1.1106,
      "step": 520
    },
    {
      "epoch": 1.0211946050096339,
      "grad_norm": 16.7353458404541,
      "learning_rate": 1.9432355723746455e-05,
      "loss": 1.2686,
      "step": 530
    },
    {
      "epoch": 1.0404624277456647,
      "grad_norm": 21.122045516967773,
      "learning_rate": 1.924314096499527e-05,
      "loss": 1.2182,
      "step": 540
    },
    {
      "epoch": 1.0597302504816957,
      "grad_norm": 21.372201919555664,
      "learning_rate": 1.9053926206244088e-05,
      "loss": 1.3181,
      "step": 550
    },
    {
      "epoch": 1.0789980732177264,
      "grad_norm": 32.59337615966797,
      "learning_rate": 1.8864711447492906e-05,
      "loss": 1.1758,
      "step": 560
    },
    {
      "epoch": 1.0982658959537572,
      "grad_norm": 12.95024585723877,
      "learning_rate": 1.8675496688741724e-05,
      "loss": 1.1647,
      "step": 570
    },
    {
      "epoch": 1.117533718689788,
      "grad_norm": 17.402019500732422,
      "learning_rate": 1.8486281929990542e-05,
      "loss": 1.2214,
      "step": 580
    },
    {
      "epoch": 1.1368015414258188,
      "grad_norm": 21.05204200744629,
      "learning_rate": 1.8297067171239356e-05,
      "loss": 1.2264,
      "step": 590
    },
    {
      "epoch": 1.1560693641618498,
      "grad_norm": 15.518013954162598,
      "learning_rate": 1.8107852412488174e-05,
      "loss": 1.1662,
      "step": 600
    },
    {
      "epoch": 1.1753371868978806,
      "grad_norm": 8.771515846252441,
      "learning_rate": 1.7918637653736992e-05,
      "loss": 1.2937,
      "step": 610
    },
    {
      "epoch": 1.1946050096339114,
      "grad_norm": 10.275175094604492,
      "learning_rate": 1.772942289498581e-05,
      "loss": 1.2218,
      "step": 620
    },
    {
      "epoch": 1.2138728323699421,
      "grad_norm": 11.251815795898438,
      "learning_rate": 1.7540208136234628e-05,
      "loss": 1.1777,
      "step": 630
    },
    {
      "epoch": 1.2331406551059731,
      "grad_norm": 6.669099807739258,
      "learning_rate": 1.7350993377483446e-05,
      "loss": 1.165,
      "step": 640
    },
    {
      "epoch": 1.252408477842004,
      "grad_norm": 3.119745969772339,
      "learning_rate": 1.7161778618732264e-05,
      "loss": 1.2575,
      "step": 650
    },
    {
      "epoch": 1.2716763005780347,
      "grad_norm": 2.6760144233703613,
      "learning_rate": 1.697256385998108e-05,
      "loss": 1.0148,
      "step": 660
    },
    {
      "epoch": 1.2909441233140655,
      "grad_norm": 3.274334192276001,
      "learning_rate": 1.6783349101229897e-05,
      "loss": 1.2336,
      "step": 670
    },
    {
      "epoch": 1.3102119460500963,
      "grad_norm": 5.98309850692749,
      "learning_rate": 1.6594134342478715e-05,
      "loss": 1.2053,
      "step": 680
    },
    {
      "epoch": 1.3294797687861273,
      "grad_norm": 6.447481155395508,
      "learning_rate": 1.6404919583727533e-05,
      "loss": 1.3016,
      "step": 690
    },
    {
      "epoch": 1.348747591522158,
      "grad_norm": 14.914405822753906,
      "learning_rate": 1.621570482497635e-05,
      "loss": 1.3413,
      "step": 700
    },
    {
      "epoch": 1.3680154142581888,
      "grad_norm": 5.219298839569092,
      "learning_rate": 1.6026490066225165e-05,
      "loss": 1.251,
      "step": 710
    },
    {
      "epoch": 1.3872832369942196,
      "grad_norm": 9.948593139648438,
      "learning_rate": 1.5837275307473986e-05,
      "loss": 0.8498,
      "step": 720
    },
    {
      "epoch": 1.4065510597302504,
      "grad_norm": 10.137024879455566,
      "learning_rate": 1.56480605487228e-05,
      "loss": 1.2503,
      "step": 730
    },
    {
      "epoch": 1.4258188824662814,
      "grad_norm": 23.266035079956055,
      "learning_rate": 1.545884578997162e-05,
      "loss": 1.0032,
      "step": 740
    },
    {
      "epoch": 1.4450867052023122,
      "grad_norm": 10.30461311340332,
      "learning_rate": 1.5269631031220437e-05,
      "loss": 1.0758,
      "step": 750
    },
    {
      "epoch": 1.464354527938343,
      "grad_norm": 13.387009620666504,
      "learning_rate": 1.5080416272469253e-05,
      "loss": 1.1639,
      "step": 760
    },
    {
      "epoch": 1.4836223506743738,
      "grad_norm": 10.739779472351074,
      "learning_rate": 1.4891201513718073e-05,
      "loss": 0.9573,
      "step": 770
    },
    {
      "epoch": 1.5028901734104045,
      "grad_norm": 15.623394012451172,
      "learning_rate": 1.4701986754966889e-05,
      "loss": 0.9984,
      "step": 780
    },
    {
      "epoch": 1.5221579961464355,
      "grad_norm": 6.868142604827881,
      "learning_rate": 1.4512771996215707e-05,
      "loss": 0.986,
      "step": 790
    },
    {
      "epoch": 1.5414258188824663,
      "grad_norm": 4.096749782562256,
      "learning_rate": 1.4323557237464523e-05,
      "loss": 1.1721,
      "step": 800
    },
    {
      "epoch": 1.560693641618497,
      "grad_norm": 20.28993797302246,
      "learning_rate": 1.4134342478713341e-05,
      "loss": 1.0794,
      "step": 810
    },
    {
      "epoch": 1.579961464354528,
      "grad_norm": 4.649950981140137,
      "learning_rate": 1.3945127719962157e-05,
      "loss": 1.1198,
      "step": 820
    },
    {
      "epoch": 1.5992292870905587,
      "grad_norm": 10.071159362792969,
      "learning_rate": 1.3755912961210975e-05,
      "loss": 1.0985,
      "step": 830
    },
    {
      "epoch": 1.6184971098265897,
      "grad_norm": 4.04088020324707,
      "learning_rate": 1.3566698202459793e-05,
      "loss": 1.2192,
      "step": 840
    },
    {
      "epoch": 1.6377649325626205,
      "grad_norm": 11.342137336730957,
      "learning_rate": 1.337748344370861e-05,
      "loss": 1.1253,
      "step": 850
    },
    {
      "epoch": 1.6570327552986512,
      "grad_norm": 11.1618013381958,
      "learning_rate": 1.3188268684957428e-05,
      "loss": 1.1722,
      "step": 860
    },
    {
      "epoch": 1.6763005780346822,
      "grad_norm": 6.562044143676758,
      "learning_rate": 1.2999053926206244e-05,
      "loss": 1.0318,
      "step": 870
    },
    {
      "epoch": 1.6955684007707128,
      "grad_norm": 14.16800594329834,
      "learning_rate": 1.2809839167455063e-05,
      "loss": 1.0963,
      "step": 880
    },
    {
      "epoch": 1.7148362235067438,
      "grad_norm": 4.144528865814209,
      "learning_rate": 1.2620624408703881e-05,
      "loss": 1.0971,
      "step": 890
    },
    {
      "epoch": 1.7341040462427746,
      "grad_norm": 13.979485511779785,
      "learning_rate": 1.2431409649952698e-05,
      "loss": 1.1821,
      "step": 900
    },
    {
      "epoch": 1.7533718689788054,
      "grad_norm": 7.599876880645752,
      "learning_rate": 1.2242194891201516e-05,
      "loss": 1.2304,
      "step": 910
    },
    {
      "epoch": 1.7726396917148364,
      "grad_norm": 14.6217679977417,
      "learning_rate": 1.2052980132450332e-05,
      "loss": 0.9031,
      "step": 920
    },
    {
      "epoch": 1.791907514450867,
      "grad_norm": 5.852708339691162,
      "learning_rate": 1.186376537369915e-05,
      "loss": 1.0619,
      "step": 930
    },
    {
      "epoch": 1.811175337186898,
      "grad_norm": 8.71501350402832,
      "learning_rate": 1.1674550614947966e-05,
      "loss": 1.0173,
      "step": 940
    },
    {
      "epoch": 1.8304431599229287,
      "grad_norm": 3.5620572566986084,
      "learning_rate": 1.1485335856196784e-05,
      "loss": 1.1032,
      "step": 950
    },
    {
      "epoch": 1.8497109826589595,
      "grad_norm": 10.31287670135498,
      "learning_rate": 1.1296121097445602e-05,
      "loss": 1.0799,
      "step": 960
    },
    {
      "epoch": 1.8689788053949905,
      "grad_norm": 3.304008722305298,
      "learning_rate": 1.1106906338694418e-05,
      "loss": 0.9621,
      "step": 970
    },
    {
      "epoch": 1.888246628131021,
      "grad_norm": 6.720941066741943,
      "learning_rate": 1.0917691579943238e-05,
      "loss": 1.0991,
      "step": 980
    },
    {
      "epoch": 1.907514450867052,
      "grad_norm": 12.91418743133545,
      "learning_rate": 1.0728476821192052e-05,
      "loss": 0.8759,
      "step": 990
    },
    {
      "epoch": 1.9267822736030829,
      "grad_norm": 6.608465194702148,
      "learning_rate": 1.0539262062440872e-05,
      "loss": 0.8558,
      "step": 1000
    },
    {
      "epoch": 1.9460500963391136,
      "grad_norm": 9.919888496398926,
      "learning_rate": 1.0350047303689688e-05,
      "loss": 1.0525,
      "step": 1010
    },
    {
      "epoch": 1.9653179190751446,
      "grad_norm": 8.244749069213867,
      "learning_rate": 1.0160832544938506e-05,
      "loss": 0.9894,
      "step": 1020
    },
    {
      "epoch": 1.9845857418111752,
      "grad_norm": 8.927614212036133,
      "learning_rate": 9.971617786187322e-06,
      "loss": 1.0052,
      "step": 1030
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.6569178852643419,
      "eval_f1": 0.46290302153434176,
      "eval_loss": 0.9644736647605896,
      "eval_precision": 0.5849995805518844,
      "eval_recall": 0.4925900563079422,
      "eval_runtime": 520.0278,
      "eval_samples_per_second": 3.419,
      "eval_steps_per_second": 0.215,
      "step": 1038
    },
    {
      "epoch": 2.003853564547206,
      "grad_norm": 5.40814733505249,
      "learning_rate": 9.78240302743614e-06,
      "loss": 0.899,
      "step": 1040
    },
    {
      "epoch": 2.023121387283237,
      "grad_norm": 7.332594394683838,
      "learning_rate": 9.593188268684958e-06,
      "loss": 0.8329,
      "step": 1050
    },
    {
      "epoch": 2.0423892100192678,
      "grad_norm": 6.6154022216796875,
      "learning_rate": 9.403973509933776e-06,
      "loss": 0.9534,
      "step": 1060
    },
    {
      "epoch": 2.0616570327552988,
      "grad_norm": 12.995237350463867,
      "learning_rate": 9.214758751182593e-06,
      "loss": 0.9405,
      "step": 1070
    },
    {
      "epoch": 2.0809248554913293,
      "grad_norm": 7.794025421142578,
      "learning_rate": 9.02554399243141e-06,
      "loss": 0.798,
      "step": 1080
    },
    {
      "epoch": 2.1001926782273603,
      "grad_norm": 9.654550552368164,
      "learning_rate": 8.836329233680227e-06,
      "loss": 0.8867,
      "step": 1090
    },
    {
      "epoch": 2.1194605009633913,
      "grad_norm": 8.308591842651367,
      "learning_rate": 8.647114474929045e-06,
      "loss": 0.8454,
      "step": 1100
    },
    {
      "epoch": 2.138728323699422,
      "grad_norm": 6.188156604766846,
      "learning_rate": 8.457899716177863e-06,
      "loss": 0.814,
      "step": 1110
    },
    {
      "epoch": 2.157996146435453,
      "grad_norm": 4.714718341827393,
      "learning_rate": 8.26868495742668e-06,
      "loss": 0.8595,
      "step": 1120
    },
    {
      "epoch": 2.1772639691714835,
      "grad_norm": 21.86362075805664,
      "learning_rate": 8.079470198675497e-06,
      "loss": 0.814,
      "step": 1130
    },
    {
      "epoch": 2.1965317919075145,
      "grad_norm": 9.3167085647583,
      "learning_rate": 7.890255439924315e-06,
      "loss": 0.8377,
      "step": 1140
    },
    {
      "epoch": 2.2157996146435455,
      "grad_norm": 11.926671981811523,
      "learning_rate": 7.701040681173133e-06,
      "loss": 0.857,
      "step": 1150
    },
    {
      "epoch": 2.235067437379576,
      "grad_norm": 18.9247989654541,
      "learning_rate": 7.511825922421949e-06,
      "loss": 0.903,
      "step": 1160
    },
    {
      "epoch": 2.254335260115607,
      "grad_norm": 14.679954528808594,
      "learning_rate": 7.322611163670766e-06,
      "loss": 0.8086,
      "step": 1170
    },
    {
      "epoch": 2.2736030828516376,
      "grad_norm": 13.7481107711792,
      "learning_rate": 7.133396404919585e-06,
      "loss": 0.8409,
      "step": 1180
    },
    {
      "epoch": 2.2928709055876686,
      "grad_norm": 12.286250114440918,
      "learning_rate": 6.944181646168402e-06,
      "loss": 0.7259,
      "step": 1190
    },
    {
      "epoch": 2.3121387283236996,
      "grad_norm": 13.813716888427734,
      "learning_rate": 6.754966887417219e-06,
      "loss": 0.9145,
      "step": 1200
    },
    {
      "epoch": 2.33140655105973,
      "grad_norm": 8.939605712890625,
      "learning_rate": 6.565752128666036e-06,
      "loss": 0.8577,
      "step": 1210
    },
    {
      "epoch": 2.350674373795761,
      "grad_norm": 8.701749801635742,
      "learning_rate": 6.376537369914853e-06,
      "loss": 0.7614,
      "step": 1220
    },
    {
      "epoch": 2.3699421965317917,
      "grad_norm": 10.25692367553711,
      "learning_rate": 6.1873226111636705e-06,
      "loss": 0.835,
      "step": 1230
    },
    {
      "epoch": 2.3892100192678227,
      "grad_norm": 10.424050331115723,
      "learning_rate": 5.998107852412489e-06,
      "loss": 0.8061,
      "step": 1240
    },
    {
      "epoch": 2.4084778420038537,
      "grad_norm": 7.5602030754089355,
      "learning_rate": 5.808893093661306e-06,
      "loss": 0.7586,
      "step": 1250
    },
    {
      "epoch": 2.4277456647398843,
      "grad_norm": 6.415134906768799,
      "learning_rate": 5.6196783349101235e-06,
      "loss": 0.8818,
      "step": 1260
    },
    {
      "epoch": 2.4470134874759153,
      "grad_norm": 7.367656230926514,
      "learning_rate": 5.430463576158941e-06,
      "loss": 0.7787,
      "step": 1270
    },
    {
      "epoch": 2.4662813102119463,
      "grad_norm": 8.661670684814453,
      "learning_rate": 5.241248817407758e-06,
      "loss": 0.6949,
      "step": 1280
    },
    {
      "epoch": 2.485549132947977,
      "grad_norm": 6.825096130371094,
      "learning_rate": 5.052034058656576e-06,
      "loss": 0.6724,
      "step": 1290
    },
    {
      "epoch": 2.504816955684008,
      "grad_norm": 9.467839241027832,
      "learning_rate": 4.862819299905393e-06,
      "loss": 0.8604,
      "step": 1300
    },
    {
      "epoch": 2.5240847784200384,
      "grad_norm": 9.327455520629883,
      "learning_rate": 4.67360454115421e-06,
      "loss": 0.7891,
      "step": 1310
    },
    {
      "epoch": 2.5433526011560694,
      "grad_norm": 6.4914937019348145,
      "learning_rate": 4.484389782403028e-06,
      "loss": 0.694,
      "step": 1320
    },
    {
      "epoch": 2.5626204238921,
      "grad_norm": 14.050402641296387,
      "learning_rate": 4.295175023651845e-06,
      "loss": 0.6817,
      "step": 1330
    },
    {
      "epoch": 2.581888246628131,
      "grad_norm": 10.067134857177734,
      "learning_rate": 4.105960264900663e-06,
      "loss": 0.8437,
      "step": 1340
    },
    {
      "epoch": 2.601156069364162,
      "grad_norm": 10.225238800048828,
      "learning_rate": 3.91674550614948e-06,
      "loss": 0.8152,
      "step": 1350
    },
    {
      "epoch": 2.6204238921001926,
      "grad_norm": 7.583691596984863,
      "learning_rate": 3.7275307473982975e-06,
      "loss": 0.7218,
      "step": 1360
    },
    {
      "epoch": 2.6396917148362236,
      "grad_norm": 10.473721504211426,
      "learning_rate": 3.5383159886471146e-06,
      "loss": 0.7728,
      "step": 1370
    },
    {
      "epoch": 2.6589595375722546,
      "grad_norm": 7.039237022399902,
      "learning_rate": 3.3491012298959317e-06,
      "loss": 0.9,
      "step": 1380
    },
    {
      "epoch": 2.678227360308285,
      "grad_norm": 8.195626258850098,
      "learning_rate": 3.1598864711447496e-06,
      "loss": 0.7373,
      "step": 1390
    },
    {
      "epoch": 2.697495183044316,
      "grad_norm": 9.47213363647461,
      "learning_rate": 2.9706717123935667e-06,
      "loss": 0.7045,
      "step": 1400
    },
    {
      "epoch": 2.7167630057803467,
      "grad_norm": 9.211609840393066,
      "learning_rate": 2.7814569536423843e-06,
      "loss": 0.7358,
      "step": 1410
    },
    {
      "epoch": 2.7360308285163777,
      "grad_norm": 7.627692222595215,
      "learning_rate": 2.592242194891202e-06,
      "loss": 0.7441,
      "step": 1420
    },
    {
      "epoch": 2.7552986512524082,
      "grad_norm": 7.889011383056641,
      "learning_rate": 2.403027436140019e-06,
      "loss": 0.6933,
      "step": 1430
    },
    {
      "epoch": 2.7745664739884393,
      "grad_norm": 7.000759124755859,
      "learning_rate": 2.2138126773888364e-06,
      "loss": 0.6619,
      "step": 1440
    },
    {
      "epoch": 2.7938342967244703,
      "grad_norm": 6.329264163970947,
      "learning_rate": 2.024597918637654e-06,
      "loss": 0.6874,
      "step": 1450
    },
    {
      "epoch": 2.813102119460501,
      "grad_norm": 9.206036567687988,
      "learning_rate": 1.8353831598864713e-06,
      "loss": 0.7024,
      "step": 1460
    },
    {
      "epoch": 2.832369942196532,
      "grad_norm": 19.25456428527832,
      "learning_rate": 1.6461684011352888e-06,
      "loss": 0.9081,
      "step": 1470
    },
    {
      "epoch": 2.851637764932563,
      "grad_norm": 15.702452659606934,
      "learning_rate": 1.456953642384106e-06,
      "loss": 0.6908,
      "step": 1480
    },
    {
      "epoch": 2.8709055876685934,
      "grad_norm": 16.97808265686035,
      "learning_rate": 1.2677388836329234e-06,
      "loss": 0.8516,
      "step": 1490
    },
    {
      "epoch": 2.8901734104046244,
      "grad_norm": 13.404434204101562,
      "learning_rate": 1.078524124881741e-06,
      "loss": 0.7761,
      "step": 1500
    },
    {
      "epoch": 2.909441233140655,
      "grad_norm": 14.595081329345703,
      "learning_rate": 8.893093661305583e-07,
      "loss": 0.7344,
      "step": 1510
    },
    {
      "epoch": 2.928709055876686,
      "grad_norm": 4.001221179962158,
      "learning_rate": 7.000946073793756e-07,
      "loss": 0.688,
      "step": 1520
    },
    {
      "epoch": 2.9479768786127165,
      "grad_norm": 7.767495155334473,
      "learning_rate": 5.10879848628193e-07,
      "loss": 0.7247,
      "step": 1530
    },
    {
      "epoch": 2.9672447013487475,
      "grad_norm": 4.903594017028809,
      "learning_rate": 3.2166508987701047e-07,
      "loss": 0.7282,
      "step": 1540
    },
    {
      "epoch": 2.9865125240847785,
      "grad_norm": 11.05090618133545,
      "learning_rate": 1.3245033112582784e-07,
      "loss": 0.8273,
      "step": 1550
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.750843644544432,
      "eval_f1": 0.6395258057827344,
      "eval_loss": 0.7760022282600403,
      "eval_precision": 0.6641352398798508,
      "eval_recall": 0.6329197398932876,
      "eval_runtime": 523.9026,
      "eval_samples_per_second": 3.394,
      "eval_steps_per_second": 0.214,
      "step": 1557
    }
  ],
  "logging_steps": 10,
  "max_steps": 1557,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3296009051283456.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
